{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras quasi-SVM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJVaxAJM3wj2",
        "colab_type": "text"
      },
      "source": [
        "# A Quasi-SVM in Keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZPU5WGO0FzQ",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This example demonstrates how to train a Keras model that approximates a Support Vector Machine (SVM).\n",
        "\n",
        "The key idea is to stack a `RandomFourierFeatures` layer with a linear layer.\n",
        "\n",
        "The `RandomFourierFeatures` layer can be used to \"kernelize\" linear models by applying a non-linear transformation to the input\n",
        "features and then training a linear model on top of the transformed features. Depending on the loss function of the linear model, the composition of this layer and the linear model results to models that are equivalent (up to approximation) to kernel SVMs (for hinge loss), kernel logistic regression (for logistic loss), kernel linear regression (for MSE loss), etc.\n",
        "\n",
        "In our case, we approximate SVM using a hinge loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKlSfFiePI1Y",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm-WoEHK1nLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import RandomFourierFeatures "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwZm92GCPOT9",
        "colab_type": "text"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXwzoO72zjoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "  keras.Input(shape=(784,)),\n",
        "  RandomFourierFeatures(\n",
        "    output_dim=4096,\n",
        "    scale=10.,\n",
        "    kernel_initializer='gaussian'),\n",
        "  layers.Dense(units=10),\n",
        "])\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=keras.losses.hinge,\n",
        "    metrics=[keras.metrics.CategoricalAccuracy(name='acc')]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6sOS639PQzd",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTZyE1cV02yP",
        "colab_type": "code",
        "outputId": "f33efa8d-a414-4ec4-d36f-a941b6e15f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Load MNIST\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Preprocess the data by flattening & scaling it\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255\n",
        "\n",
        "# Categorical (one hot) encoding of the labels\n",
        "y_train = keras.utils.to_categorical(y_train)\n",
        "y_test = keras.utils.to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBaM2AE5PSe2",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OTz81tA1Oau",
        "colab_type": "code",
        "outputId": "7a488b43-8a38-40b3-8253-966113a6a392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "model.fit(x_train, y_train, epochs=20, batch_size=128, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "375/375 [==============================] - 11s 31ms/step - loss: 0.0873 - acc: 0.8620 - val_loss: 0.0450 - val_acc: 0.9332\n",
            "Epoch 2/20\n",
            "375/375 [==============================] - 12s 32ms/step - loss: 0.0418 - acc: 0.9380 - val_loss: 0.0358 - val_acc: 0.9481\n",
            "Epoch 3/20\n",
            "375/375 [==============================] - 12s 32ms/step - loss: 0.0341 - acc: 0.9500 - val_loss: 0.0350 - val_acc: 0.9507\n",
            "Epoch 4/20\n",
            "375/375 [==============================] - 12s 32ms/step - loss: 0.0287 - acc: 0.9587 - val_loss: 0.0305 - val_acc: 0.9579\n",
            "Epoch 5/20\n",
            "375/375 [==============================] - 12s 32ms/step - loss: 0.0256 - acc: 0.9645 - val_loss: 0.0302 - val_acc: 0.9551\n",
            "Epoch 6/20\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.0234 - acc: 0.9678 - val_loss: 0.0291 - val_acc: 0.9590\n",
            "Epoch 7/20\n",
            "375/375 [==============================] - 12s 32ms/step - loss: 0.0217 - acc: 0.9709 - val_loss: 0.0262 - val_acc: 0.9624\n",
            "Epoch 8/20\n",
            "375/375 [==============================] - 12s 32ms/step - loss: 0.0201 - acc: 0.9729 - val_loss: 0.0237 - val_acc: 0.9682\n",
            "Epoch 9/20\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.0188 - acc: 0.9748 - val_loss: 0.0240 - val_acc: 0.9653\n",
            "Epoch 10/20\n",
            "375/375 [==============================] - 12s 32ms/step - loss: 0.0179 - acc: 0.9768 - val_loss: 0.0232 - val_acc: 0.9683\n",
            "Epoch 11/20\n",
            "375/375 [==============================] - 12s 32ms/step - loss: 0.0164 - acc: 0.9793 - val_loss: 0.0254 - val_acc: 0.9654\n",
            "Epoch 12/20\n",
            "375/375 [==============================] - 12s 32ms/step - loss: 0.0157 - acc: 0.9801 - val_loss: 0.0221 - val_acc: 0.9703\n",
            "Epoch 13/20\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.0153 - acc: 0.9811 - val_loss: 0.0241 - val_acc: 0.9676\n",
            "Epoch 14/20\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.0145 - acc: 0.9825 - val_loss: 0.0216 - val_acc: 0.9703\n",
            "Epoch 15/20\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.0139 - acc: 0.9834 - val_loss: 0.0210 - val_acc: 0.9720\n",
            "Epoch 16/20\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.0134 - acc: 0.9844 - val_loss: 0.0230 - val_acc: 0.9696\n",
            "Epoch 17/20\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.0126 - acc: 0.9851 - val_loss: 0.0250 - val_acc: 0.9670\n",
            "Epoch 18/20\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.0124 - acc: 0.9860 - val_loss: 0.0218 - val_acc: 0.9707\n",
            "Epoch 19/20\n",
            "375/375 [==============================] - 12s 31ms/step - loss: 0.0111 - acc: 0.9874 - val_loss: 0.0241 - val_acc: 0.9690\n",
            "Epoch 20/20\n",
            "375/375 [==============================] - 11s 31ms/step - loss: 0.0110 - acc: 0.9872 - val_loss: 0.0249 - val_acc: 0.9662\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f55af82efd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGj4zvrp3ht2",
        "colab_type": "text"
      },
      "source": [
        "I can't say that it works well or that it is indeed a good idea, but you can probably get decent results by tuning your hyperparameters.\n",
        "\n",
        "You can use this setup to add a \"SVM layer\" on top of a deep learning model, and train the whole model end-to-end."
      ]
    }
  ]
}